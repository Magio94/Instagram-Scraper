{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267110b3",
   "metadata": {},
   "source": [
    "# A jupyter notebook to scrape data on Instagram\n",
    "\n",
    "This notebook was written for scraping data from Instagram posts based on a hashtag or location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c34ca",
   "metadata": {},
   "source": [
    "## Import packages needed for the scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed4c041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random, pandas as pd, json, traceback\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8425395",
   "metadata": {},
   "source": [
    "## Download ChromeDriver\n",
    "\n",
    "To be able to scrap it is necessary to simulate a Browser through the code and open Instagram in it. To do this, you must initially download a driver that the code will use to simulate the Browser.\n",
    "\n",
    "To download the ChromeDriver click on this link:\n",
    "<br>\n",
    "https://chromedriver.chromium.org/\n",
    "\n",
    "Once the driver has been downloaded, insert it in a PATH that must be indicated in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9c301",
   "metadata": {},
   "source": [
    "## Open Instagram with ChromeDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53960c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the path to chromedriver.exe (download and save on your computer and insert the path)\n",
    "driver = Chrome(r\"/Users/magio94/Dropbox/Master of Science in Geospatial Technologies/Corsi Muenster/Transferring Data to Knowledge/Project/chromedriver\")\n",
    "\n",
    "#open the webpage\n",
    "driver.get(\"http://www.instagram.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d749d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#close the pop-up\n",
    "button0 = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), \"Accetta tutti\")]'))).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c93ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target username\n",
    "username = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='username']\")))\n",
    "password = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"input[name='password']\")))\n",
    "\n",
    "#enter username and password\n",
    "username.clear()\n",
    "username.send_keys(\"put here you username\")\n",
    "\n",
    "password.clear()\n",
    "password.send_keys(\"put here your password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90155dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "\n",
    "#target the login button and click it\n",
    "button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[type='submit']\"))).click()\n",
    "\n",
    "#We are logged in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5abf6588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#close the NOT NOW pop-ups\n",
    "time.sleep(5)\n",
    "not_now = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), \"Non ora\")]'))).click()\n",
    "time.sleep(2)\n",
    "not_now2 = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), \"Non ora\")]'))).click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0558195",
   "metadata": {},
   "source": [
    "## Search the hashtag (or it can be used for find location, users, etc. posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae4cea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#target the search input field\n",
    "searchbox = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//input[@placeholder='Cerca']\")))\n",
    "searchbox.clear()\n",
    "\n",
    "#search for the hashtag cat\n",
    "keyword = \"#valborbera\"\n",
    "searchbox.send_keys(keyword)\n",
    " \n",
    "# Wait for 5 seconds\n",
    "time.sleep(5)\n",
    "searchbox.send_keys(Keys.ENTER)\n",
    "time.sleep(5)\n",
    "searchbox.send_keys(Keys.ENTER)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e3f80",
   "metadata": {},
   "source": [
    "## Scrape posts\n",
    "\n",
    "The following code is written for:\n",
    "- select the first post of the research done;\n",
    "- create the lists in which to save the information I want to obtain for each post (in this case: 1) link of each post, 2) date of publication of the post, 3) the hashtags contained in the post, 4) the link of the location linked to the post, 5) the link of the user puplished the post\n",
    "- finally the last code reads the post and then passes to the following one, until it encounters some error, at this point the program stops and waits for information to be obtained again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bef23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click the first post of the search\n",
    "click_first_post = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//*[@id='react-root']/section/main/article/div[1]/div/div/div[1]/div[1]/a/div/div[2]\"))).click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8992d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list for the info to scrape\n",
    "links = []\n",
    "date_time = []\n",
    "hashtags_by_posts = []\n",
    "lst_locations = []\n",
    "users = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90323d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# scrape the data\n",
    "\n",
    "i = 0\n",
    "time.sleep(2)\n",
    "\n",
    "while True:\n",
    "    i+=1\n",
    "    try:\n",
    "        # get the user\n",
    "        user = driver.find_element(By.XPATH,'/html/body/div[5]/div[2]/div/article/header/div[2]/div[1]/div/span/a')\n",
    "        user_attribute = user.get_attribute('href')\n",
    "        users.append(user_attribute)\n",
    "        \n",
    "        # get the links\n",
    "        get_link = driver.current_url\n",
    "        links.append(get_link)\n",
    "        # get time\n",
    "        get_date_time = driver.find_element(By.XPATH,'/html/body/div[5]/div[2]/div/article/div[3]/div[2]/a/time').text\n",
    "        date_time.append(get_date_time)\n",
    "        # get the hashtags\n",
    "        links2 = driver.find_elements(By.XPATH,'/html/body/div[5]/div[2]/div/article/div[3]/div[1]/ul/div/li/div/div/div[2]/span/a')        \n",
    "        hashtags = []\n",
    "        for link2 in links2:\n",
    "            hashtags.append((link2.text))\n",
    "        hashtags_by_posts.append(hashtags)\n",
    "        # get the location\n",
    "        try:\n",
    "            location = driver.find_element(By.XPATH,'/html/body/div[5]/div[2]/div/article/header/div[2]/div[2]/div[2]/a')\n",
    "            lst_locations.append(location.get_attribute('href'))\n",
    "        except:\n",
    "            lst_locations.append(0)\n",
    "        scroll_post = driver.find_element_by_css_selector('a._65Bje.coreSpriteRightPaginationArrow').click()\n",
    "        # avoid errors\n",
    "        while True:\n",
    "            try:\n",
    "                element = WebDriverWait(driver, 20).until(\n",
    "                    EC.presence_of_element_located((\n",
    "                        By.XPATH,'/html/body/div[5]/div[2]/div/article/div[3]/div[2]/a/time')))\n",
    "                break\n",
    "            except:\n",
    "                scroll_back_post = driver.find_element_by_css_selector('a.ITLxV.coreSpriteLeftPaginationArrow').click()\n",
    "                time.sleep(10)\n",
    "                while True:\n",
    "                    try:\n",
    "                        element = WebDriverWait(driver, 20).until(\n",
    "                            EC.presence_of_element_located((\n",
    "                                By.XPATH,'/html/body/div[5]/div[2]/div/article/div[3]/div[2]/a/time')))\n",
    "                        scroll_post = driver.find_element_by_css_selector('a._65Bje.coreSpriteRightPaginationArrow').click()\n",
    "                        time.sleep(3)\n",
    "                        break\n",
    "                    except:\n",
    "                        continue \n",
    "        time_sleep = float(random.randrange(450, 550))/100\n",
    "        time.sleep(time_sleep)\n",
    "    except Exception:\n",
    "        print(i)\n",
    "        traceback.print_exc()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eedf920",
   "metadata": {},
   "source": [
    "## Check if the list have the same number of objects to merge them after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70297723",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c3638",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hashtags_by_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa4db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lst_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7712a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec8febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Links' : links, 'Date_time' : date_time, 'Hashtags_by_posts' : hashtags_by_posts, \n",
    "                   'Geotag' : lst_locations, 'users' : users})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2487b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8110cdda",
   "metadata": {},
   "source": [
    "## Save the each list and the final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('links4.txt', 'w') as f:\n",
    "    for item in links:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('date_time4.txt', 'w') as f:\n",
    "    for item in date_time:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e92aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hashtags_by_posts4.txt', 'w') as f:\n",
    "    for item in hashtags_by_posts:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f93a931",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lst_locations4.txt', 'w') as f:\n",
    "    for item in lst_locations:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9def02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('users4.txt', 'w') as f:\n",
    "    for item in lst_locations:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8c6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('merged_lists.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eaa41c",
   "metadata": {},
   "source": [
    "## Scrape the coordinates by location from the links got in the post\n",
    "\n",
    "Once you have obtained the links of the place where the post was published in case it was available, to obtain the coordinates it is necessary to insert the link for each location in the browser. There is a possibility that many of the locations have been collected more than once, so to reduce the scraping time, create a dictionary for having no repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the file saved with the location (or you can just use the list)\n",
    "\n",
    "a_file = open(\"lst_locations4.txt\", \"r\")\n",
    "\n",
    "lst_locations = []\n",
    "\n",
    "for line in a_file:\n",
    "    stripped_line = line.strip()\n",
    "    lst_locations.append(stripped_line)\n",
    "\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc986d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list for the locations for having no repetitions and counting them\n",
    "\n",
    "geotag = list(dict.fromkeys(lst_locations))\n",
    "len(geotag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7871af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list to store the coordinates\n",
    "\n",
    "coordinates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b179ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# scrape the location\n",
    "\n",
    "for i in geotag:\n",
    "    while True:\n",
    "        try:\n",
    "            if str(i) == '0':\n",
    "                lat = 0\n",
    "                lng = 0\n",
    "            else:\n",
    "                driver.get(str(i))\n",
    "                js = \"return JSON.stringify(window._sharedData)\"\n",
    "                get_lan_lng = json.loads(driver.execute_script(js))\n",
    "                lat = get_lan_lng['entry_data']['LocationsPage'][0]['native_location_data']['location_info']['lat']\n",
    "                lng = get_lan_lng['entry_data']['LocationsPage'][0]['native_location_data']['location_info']['lng']        \n",
    "            break\n",
    "        except:\n",
    "            time_sleep = float(random.randrange(1500, 2500))/100\n",
    "            time.sleep(time_sleep)\n",
    "            \n",
    "        \n",
    "    coordinates.append([i, lat, lng])\n",
    "    time_sleep = float(random.randrange(800, 1500))/100\n",
    "    time.sleep(time_sleep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a6892b",
   "metadata": {},
   "source": [
    "### Scrape location from a determined link\n",
    "\n",
    "Instagram could block the location scraping process through the link, in fact after several calls, the page will remain blank. In this case you can make use of this code and also for this reason the list with the coordinates has been loaded separately so as not to overwrite the information already obtained, but to add new ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ded4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of the link where the code should start to scrape again\n",
    "\n",
    "start_from = int(len(coordinates)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# start the scraping from the number of the link found before\n",
    "\n",
    "for i in geotag[start_from:]:\n",
    "    while True:\n",
    "        \n",
    "        try:\n",
    "            if str(i) == '0':\n",
    "                lat = 0\n",
    "                lng = 0\n",
    "                \n",
    "            else:\n",
    "                driver.get(str(i))\n",
    "                js = \"return JSON.stringify(window._sharedData)\"\n",
    "                get_lan_lng = json.loads(driver.execute_script(js))\n",
    "                lat = get_lan_lng['entry_data']['LocationsPage'][0]['native_location_data']['location_info']['lat']\n",
    "                lng = get_lan_lng['entry_data']['LocationsPage'][0]['native_location_data']['location_info']['lng']        \n",
    "            break\n",
    "        except:\n",
    "            time_sleep = float(random.randrange(1500, 2500))/100\n",
    "            time.sleep(time_sleep)\n",
    "            continue\n",
    "        break\n",
    "            \n",
    "    coordinates.append([i, lat, lng])\n",
    "    time_sleep = float(random.randrange(800, 1500))/100\n",
    "    time.sleep(time_sleep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b22432",
   "metadata": {},
   "source": [
    "## Save the coordinates as table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdab7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table with the coordinates got with pandas\n",
    "\n",
    "export_table = pd.DataFrame(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b24501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming the columns \n",
    "\n",
    "export_table.columns = [\"Geotag\", \"lat\", \"lng\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15057d0f",
   "metadata": {},
   "source": [
    "## Join the location table with the posts table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03de4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_table = pd.merge(export_table, df, on='Geotag')\n",
    "merged_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f001533",
   "metadata": {},
   "source": [
    "## Counts the number of posts by location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2f7278",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_table = merged_table.groupby([\"Geotag\", \"lat\", \"lng\"]).size().reset_index(name='counts')\n",
    "count_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414551a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
